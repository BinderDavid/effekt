module stringtests

import data/string
import data/option

type Token {
    Number(content: String);
    Ident(content: String);
    Punct(content: String)
}

effect Next(): Option[Token]
effect EOS[A](): A
effect TokenizerError[A](msg: String): A

def next(): Token / { Next, EOS } =
    do Next().getOrElse { do EOS() }


def lexer[R](in: String) { prog: R / Next } : R / TokenizerError = {

    val numberRx = regex("^[0-9]+");
    val identRx  = regex("^[a-zA-Z]+");
    val punctRx  = regex("^[,.()\\[\\]]+");

    var str = in.trim;

    def tryMatch(regex: Regex): Option[String] =
        regex.exec(str).map { m =>
          str = str.substring(m.matched.length).trim;
          m.matched
        }

    def number() = tryMatch(numberRx).map { m => Number(m) }
    def ident()  = tryMatch(identRx).map { m => Ident(m) }
    def punct()  = tryMatch(punctRx).map { m => Punct(m) }

    try {  prog() } with {
        case Next() => if (str == "") resume(None()) else {
            val result =
                number()
                    .orElse { ident() }
                    .orElse { punct() }
                    .orElse { do TokenizerError("Cannot tokenize input: " ++ str) };
            resume(result)
        }
    }
}


def main() = {
    val r = regex("^[0-9]+");

    println("abcdefg".charAt(3));
    println("abcdefg".charAt(10));
    println("abcdefg".length);
    println(r.exec("0bcd0").map { m => m.index });
    try {
        lexer("abcd 12345,4 (efgh)bar(baz)") {
            while (true) {
                println(next())
            }
        }
    } with {
        case TokenizerError(msg) => println(msg)
        case EOS() => println("End of Stream")
    }
}