module stringtests

import data/string
import data/option

type Token {
    Number(content: String);
    Ident(content: String);
    Punct(content: String)
}

effect Peek(): Option[Token]
effect Next(): Token
effect EOS[A](): A

effect Lexer = { Peek, Next }

effect TokenizerError[A](msg: String): A


def peek(): Option[Token] / Lexer =
    do Peek()

def next(): Token / Lexer =
    do Next()


def lexer[R](in: String) { prog: R / Lexer } : R / TokenizerError = {

    val numberRx = regex("^[0-9]+");
    val identRx  = regex("^[a-zA-Z]+");
    val punctRx  = regex("^[,.()\\[\\]]+");

    var str = in.trim;

    def tryMatch(regex: Regex) { f: String => Token } : Option[(Token, String)] =
        regex.exec(str).map { m =>
          val rest = str.substring(m.matched.length).trim;
          (f(m.matched), rest)
        }

    def number() = tryMatch(numberRx) { m => Number(m) }
    def ident()  = tryMatch(identRx) { m => Ident(m) }
    def punct()  = tryMatch(punctRx) { m => Punct(m) }

    def lexeme() = number().orElse { ident() }.orElse { punct() }

    try {  prog() } with {
        case Peek() => if (str == "") resume(None()) else resume(lexeme().map { m => m.first })
        case Next() => if (str == "") do TokenizerError("Unexpected EOS") else {
            val result = lexeme()
                .map { m => str = m.second; m.first }
                .getOrElse { do TokenizerError("Cannot tokenize input: " ++ str) };

            resume(result)
        }
    }
}

// parsing
effect Flip(): Boolean
effect Fail[A](msg: String): A

effect Parser = { Flip, Fail, Next }

def tokenErrorToFail[R] { p: R / TokenizerError } : R / Fail = try { p() } with {
    case TokenizerError(msg) => do Fail(msg)
}

type ParseResult[R] {
  Success(t: R);
  Failure(msg: String);
  ParseError(msg: String)
}

def accept { p: Token => Boolean } : Token / { Fail, Next } = {
    val got = do Next();
    if (p(got)) {
        got
    } else {
        do Fail("Unexpected token " ++ show(got))
    }
}
def ident() = accept { t => t match {
    case Ident(content) => true
    case Number(content) => false
    case Punct(content) => false
}}
def number() = accept { t => t match {
    case Ident(content) => false
    case Number(content) => true
    case Punct(content) => false
}}
def punct(p: String) = accept { t => t match {
    case Ident(content) => false
    case Number(content) => false
    case Punct(content) => content == p
}}

def or[R] { p: R } { q: R } =
  if (do Flip()) { p() } else { q() }

def opt[R] { p: R }: Option[R] / Flip =
   or { Some(p()) } { None() }

def eager[R] { p: R / { Flip, Fail } }: ParseResult[R] = try {
  Success(p())
} with {
  case Flip() => resume(true) match {
    case Failure(msg) => resume(false)
    case Success(res) => Success(res)
    case ParseError(msg) => ParseError(msg)
  }
  case Fail(msg) => Failure(msg)
}

def parse[R](s: String) { p : R / Parser } =
    eager {
        tokenErrorToFail {
            lexer(s) {
                p()
            }
        }
    }

// Example grammar
// <EXPR> ::= <NUMBER> | <IDENT> `(` <EXPR> `,` <EXPR>  `)`
// we count the number of nodes as semantic action
def parseExpr(): Int / Parser =
    or { number(); 1 } {
        var count = 1;
        ident();
        punct("(");
        count = count + parseExpr();
        punct(",");
        count = count + parseExpr();
        punct(")");
        count
    }

def main() = {
    // simple string ops
    val r = regex("^[0-9]+");
    println("abcdefg".charAt(3));
    println("abcdefg".charAt(10));
    println("abcdefg".length);
    println(r.exec("0bcd0").map { m => m.index });

    // lexer
    try {
        lexer("abcd 12345,4 (efgh)bar(baz)") {
            while (peek().isDefined) {
                println(next())
            }
        }
    } with {
        case TokenizerError(msg) => println(msg)
    };

    // parser
    println(parse("42") { parseExpr() });
    println(parse("foo(1)") { parseExpr() });
    println(parse("foo(1, 2)") { parseExpr() })
}