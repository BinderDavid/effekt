module stringtests

import data/string
import data/option

record Position(line: Int, col: Int, index: Int)

type Token {
    Number(content: String, pos: Position);
    Ident(content: String, pos: Position);
    Punct(content: String, pos: Position)
}

effect Peek(): Option[Token]
effect Next(): Token
effect EOS[A](): A
effect TokenizerError[A](msg: String, pos: Position): A

effect Lexer = { Peek, Next }

def peek(): Option[Token] / Lexer =
    do Peek()

def next(): Token / Lexer =
    do Next()


def lexer[R](in: String) { prog: R / Lexer } : R / TokenizerError = {

    val numberRx = regex("^[0-9]+");
    val identRx  = regex("^[a-zA-Z]+");
    val punctRx  = regex("^[,.()\\[\\]{}:]");
    val spaceRx  = regex("^[ \t]+");
    val lineRx  = regex("^[\n]");

    var index = 0;
    var col = 1;
    var line = 1;

    def position() = Position(line, col, index)
    def input() = in.substring(index)

    def consume(len: Int): Unit = {
        col = col + len;
        index = index + len
    }

    def eos(): Boolean = index >= in.length

    def skipWhitespaces(): Unit =  {
        spaceRx.exec(input()).map { m =>
            consume(m.matched.length)
        }.orElse {
            lineRx.exec(input()).map { m =>
                consume(m.matched.length);
                line = line + 1;
                col = 1
            }
        }.map { m => skipWhitespaces() };
        ()
    }

    def tryMatch(regex: Regex) { f: String => Token } : Option[(Token, Int)] =
        regex.exec(input()).map { m =>
          (f(m.matched), m.matched.length)
        }

    def number() = tryMatch(numberRx) { m => Number(m, position()) }
    def ident()  = tryMatch(identRx) { m => Ident(m, position()) }
    def punct()  = tryMatch(punctRx) { m => Punct(m, position()) }

    def lexeme() = number().orElse { ident() }.orElse { punct() }

    // skip initial whitespace
    skipWhitespaces();

    try {  prog() } with {
        case Peek() =>
            if (eos())
                resume(None())
            else
                resume(lexeme().map { m => m.first })

        case Next() =>
            if (eos())  {
                do TokenizerError("Unexpected EOS", position())
            } else {
                val result = lexeme()
                    .map { m =>
                        consume(m.second);
                        skipWhitespaces();
                        m.first
                    }
                    .getOrElse {
                        do TokenizerError("Cannot tokenize input", position())
                    };

                resume(result)
            }
    }
}

// parsing
effect Flip(): Boolean
effect Fail[A](msg: String): A

effect Parser = { Flip, Fail, Next }

def tokenErrorToFail[R] { p: R / TokenizerError } : R / Fail = try { p() } with {
    case TokenizerError(msg, pos) => do Fail(msg)
}

type ParseResult[R] {
  Success(t: R);
  Failure(msg: String)
}

def accept { p: Token => Boolean } : Token / { Fail, Next } = {
    val got = do Next();
    if (p(got)) {
        got
    } else {
        do Fail("Unexpected token " ++ show(got))
    }
}
def ident() = accept { t => t match {
    case Ident(content, pos) => true
    case Number(content, pos) => false
    case Punct(content, pos) => false
}}
def number() = accept { t => t match {
    case Ident(content, pos) => false
    case Number(content, pos) => true
    case Punct(content, pos) => false
}}
def punct(p: String) = accept { t => t match {
    case Ident(content, pos) => false
    case Number(content, pos) => false
    case Punct(content, pos) => content == p
}}

def or[R] { p: R } { q: R } =
  if (do Flip()) { p() } else { q() }

def opt[R] { p: R }: Option[R] / Flip =
   or { Some(p()) } { None() }

def eager[R] { p: R / { Flip, Fail } }: ParseResult[R] = try {
  Success(p())
} with {
  case Flip() => resume(true) match {
    case Failure(msg) => resume(false)
    case Success(res) => Success(res)
  }
  case Fail(msg) => Failure(msg)
}

def parse[R](s: String) { p : R / Parser } =
    eager {
        tokenErrorToFail {
            lexer(s) {
                p()
            }
        }
    }

def many { p: Unit }: Unit / Flip =
    or { p(); many { p() } } { () }

def some { p: Unit }: Unit / Flip = {
    p(); many { p() }
}

// Example grammar
// <EXPR> ::= <NUMBER> | <IDENT> `(` <EXPR> (`,` <EXPR>)*  `)`
// we count the number of nodes as semantic action
def parseExpr(): Int / Parser =
    or { number(); 1 } {
        var count = 1;
        ident();
        punct("(");
        count = count + parseExpr();
        many {
            punct(",");
            count = count + parseExpr()
        };
        punct(")");
        count
    }


def main() = {
    // simple string ops
    val r = regex("^[0-9]+");
    println("abcdefg".charAt(3));
    println("abcdefg".charAt(10));
    println("abcdefg".length);
    println(r.exec("0bcd0").map { m => m.index });

    // lexer
    try {
        lexer("abcd 12345,4\n(efgh)bar(baz)") {
            while (peek().isDefined) {
                println(next())
            }
        }
    } with {
        case TokenizerError(msg, pos) => pos match {
            case Position(line, col, idx) =>
                println(show(line) ++ ":" ++ show(col) ++ " " ++ msg)
        }
    };

    // parser
    println(parse("42") { parseExpr() });
    println(parse("foo(1)") { parseExpr() });
    println(parse("foo(1, 2)") { parseExpr() });
    println(parse("foo(1, 2, 3, 4)") { parseExpr() });
    println(parse("foo(1, 2, bar(4, 5))") { parseExpr() });
    println(parse("foo(1, 2,\nbar(4, 5))") { parseExpr() })
}